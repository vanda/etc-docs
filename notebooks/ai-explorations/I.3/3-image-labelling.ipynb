{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6d82f9-026a-4af3-9385-586919865261",
   "metadata": {},
   "source": [
    "# 2 - Image Labelling (Sample Implementation)\n",
    "\n",
    "(Note - this is a worked through example of the problem and not neccessarily the best approach to implement - see the full challenge overview and related research papers/implementations here)\n",
    "\n",
    "    \n",
    "## Challenge\n",
    "\n",
    "## Sample Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7d4c7-8d96-4ff2-b1c2-850deaf809f4",
   "metadata": {},
   "source": [
    "## Simple topk labelling\n",
    "\n",
    "This very basic approach just takes the top 5 labels from the model classification results and output them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22770ba7-a8a4-4e21-964c-e5da3c90714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "landseer_url = \"https://framemark.vam.ac.uk/collections/2006AU1447/full/full/0/default.jpg\"\n",
    "landseer_image_pil = Image.open(requests.get(landseer_url, stream=True).raw)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.PILToTensor()])\n",
    "\n",
    "dog_image_torch = transform(landseer_image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d305b49-a50c-4386-bc29-488115e53b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(dog_image_torch).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "\n",
    "_, class_ids_topk = prediction.topk(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ea0c6c2-1a44-4ab5-829f-333a0e3677fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newfoundland\n",
      "standard poodle\n",
      "English springer\n",
      "English setter\n",
      "cocker spaniel\n"
     ]
    }
   ],
   "source": [
    "for class_id in class_ids_topk:\n",
    "    print(weights.meta[\"categories\"][class_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67641c9a-6f1a-4633-9ac6-4f3931d0a2d5",
   "metadata": {},
   "source": [
    "## Better approach\n",
    "\n",
    "The two obvious issues with the previous output are that the labels are repetative and more than required. A better approach is to \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b79367-75cc-41cd-a482-25f62032b551",
   "metadata": {},
   "source": [
    "# Dog Sculpture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de19e72e-451d-4bf0-92c7-c75d5e035207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "jacquemart_url = \"https://framemark.vam.ac.uk/collections/2016JC7374/full/full/0/default.jpg\"\n",
    "jacquemart_image_pil = Image.open(requests.get(jacquemart_url, stream=True).raw)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.PILToTensor()])\n",
    "\n",
    "dog_image_torch = transform(jacquemart_image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d675ae0d-ca46-4ed7-8adc-fa12a13aaa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedestal: 7.6%\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(dog_image_torch).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f7b64-c52a-4b07-a958-62b5644b9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit - Sample implemenetation code based on that of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df5afa-49db-4d30-83f0-5ffd645ef344",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525bd24b-4fbf-43f7-88e9-0c969f9269aa",
   "metadata": {},
   "source": [
    "# See also "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6d82f9-026a-4af3-9385-586919865261",
   "metadata": {},
   "source": [
    "# 1 - Foreground Seperation (Sample Implementation)\n",
    "\n",
    "[Please note - this is not indended to be the best approach to the problem, this is a worked example to set the problem out and how it can be solved to some extent. Better approaches may be available in and a current overview of the best in class is given.\n",
    "\n",
    "As discussed in the introduction, the seperation of the pixels representing the cultural hertiage object in a picture\n",
    "from the pixels of the background/backdrop.\n",
    "\n",
    "The model training and classes/labels that it knows about really shows up here the difference between the domains of modern photography and for detecting artworks.\n",
    "\n",
    "## Challenge\n",
    "\n",
    "Extract the cultural heritage object (painting, sculpture, kimono, etc) pixels from the background pixels.\n",
    "\n",
    "## Sample Implementation\n",
    "\n",
    "The code below is heavily based on the example segmentation code in https://pytorch.org/vision/stable/models.html#semantic-segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d788898-db06-426f-bc28-d51e6c9d00d9",
   "metadata": {},
   "source": [
    "## Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c7e40-da58-46fa-9a8b-18efaaeb83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Pillow requests torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def71e14-d787-497c-8d5c-0a1feba9ef60",
   "metadata": {},
   "source": [
    "## Model Setup (ResNet50)\n",
    "\n",
    "First we need to retrieve the pre-trained model weights, trained on the ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bdc053-91a9-4c07-aafe-6bf5d43c255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "model = fcn_resnet50(weights=weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4915ac6-9bab-4c6d-8da9-7df195657baf",
   "metadata": {},
   "source": [
    "## Trump - Hogarth's Dog (photographed against a gradient background)\n",
    "\n",
    "First test object, Hogarth's dog Trump in sculptural form . This is photographed against a gradient background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41754f99-65a1-444d-a30e-50ec7f005aed",
   "metadata": {},
   "source": [
    "We need to retrieve the image via the IIIF endpoint at the V&A and transform it into a tensor, as PyTorch expects for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d954851f-95e8-4f3e-8c7a-03a987ed922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "url = \"https://framemark.vam.ac.uk/collections/2006AT0614/full/full/0/default.jpg\"\n",
    "dog_image_pil = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.PILToTensor()])\n",
    "\n",
    "dog_image_torch = transform(dog_image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13c88a4b-2de3-4212-acc4-b4a486c37b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "model = fcn_resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(dog_image_torch).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[\"out\"]\n",
    "normalized_masks = prediction.softmax(dim=1)\n",
    "class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\n",
    "mask = normalized_masks[0, class_to_idx[\"dog\"]]\n",
    "to_pil_image(mask).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33565ed3-b987-41d8-9db1-5ebbd388d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageChops\n",
    "\n",
    "mask_image_pil = to_pil_image(mask)\n",
    "dog_image_pil_resized = dog_image_pil.resize(mask_image_pil.size)\n",
    "\n",
    "masked_image = ImageChops.multiply(dog_image_pil_resized, mask_image_pil.convert('RGB'))\n",
    "masked_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a34079-0ee6-481f-b39f-c85e6c5ad91e",
   "metadata": {},
   "source": [
    "## Lion - Landseer's Dog (dog within picture frame)\n",
    "\n",
    "Let's compare this with a dog *within* a artwork, that is a painting of a dog. Now we have three levels of segmentation\n",
    "to deal with\n",
    "\n",
    "  * The white background around the artwork (a painting, unframed in this image)\n",
    "  * The background in the painting\n",
    "  * The dog in the foreground of the painting\n",
    "\n",
    "Let see what the model makes of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04588a79-2f88-415f-a946-1cb2af2814cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "landseer_url = \"https://framemark.vam.ac.uk/collections/2006AU1447/full/full/0/default.jpg\"\n",
    "landseer_image_pil = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.PILToTensor()])\n",
    "\n",
    "dog_image_torch = transform(landseer_image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8182d4cc-90e8-4a12-bb1c-1e4043206253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "model = fcn_resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(dog_image_torch).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[\"out\"]\n",
    "normalized_masks = prediction.softmax(dim=1)\n",
    "class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\n",
    "mask = normalized_masks[0, class_to_idx[\"dog\"]]\n",
    "to_pil_image(mask).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "685ec587-74e2-4c9d-abaa-ff0dce071f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageChops\n",
    "\n",
    "mask_image_pil = to_pil_image(mask)\n",
    "dog_image_pil_resized = dog_image_pil.resize(mask_image_pil.size)\n",
    "\n",
    "masked_image = ImageChops.multiply(dog_image_pil_resized, mask_image_pil.convert('RGB'))\n",
    "masked_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9ae79-9197-46d2-bf2c-756e8f6a25b7",
   "metadata": {},
   "source": [
    "The results are much more successful at recognising the dog, but of course it has removed the rest of the painting, making this more relevant to object recognition than segmentation.\n",
    "\n",
    "This is as expected because the training data classes/labels for the model are not for cultural heritage objects, and perhaps \n",
    "training for this is something that cannot be done, because 'what is art' etc etc. Although it would seem possible to train a model specifically on common features of some artworks (paintings in frames, sculptures on a plinth, dress on a mannequin, etc)\n",
    "\n",
    "Ater working through this simple example, alternative implmentations which may be more successful are given in ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a0d3d-a9d8-475a-845d-93328af374bf",
   "metadata": {},
   "source": [
    "## Ethical Considerations\n",
    "\n",
    "  * MsCOCO dataset \n",
    "  * FCN Resnet ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4a199-e71f-4046-9dc7-e4315f3a311e",
   "metadata": {},
   "source": [
    "## Environmental Considerations\n",
    "\n",
    " * Creating the model\n",
    " * Running the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584cb55-809f-43c1-994c-2f7af53cfe4d",
   "metadata": {},
   "source": [
    "## Social/Economic Considerations\n",
    "\n",
    "If successful in this problem it will reduces work needed in graphic design tasks and related content creation which may or may not contibute to job losses in those industries. Alternatively it removes at tedious manual task and frees up time for more creative work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525bd24b-4fbf-43f7-88e9-0c969f9269aa",
   "metadata": {},
   "source": [
    "# See also "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3b90ee-757f-4b09-81e4-77004daf0a77",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "  * https://pyimagesearch.com/2020/09/28/image-segmentation-with-mask-r-cnn-grabcut-and-opencv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5b670-c7d9-4a27-bf3b-10cec6c6f907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

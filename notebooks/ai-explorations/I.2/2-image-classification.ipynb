{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6d82f9-026a-4af3-9385-586919865261",
   "metadata": {},
   "source": [
    "# 2 - Image Classification (Sample Implementation)\n",
    "\n",
    "(Note - this is a worked through example of the problem and not neccessarily the best approach to implement - see the full challenge overview and related research papers/implementations here)\n",
    "\n",
    "Perhaps the most common desire in the cultural heritage community, from an image of a cultural heritage object a text summary (of whatever desired length) can be written giving a description of the object.\n",
    "\n",
    "An even more specific form of this challenge is for the description to be clear that is of an object of which an image has been taken (so not just describing the content of the image but also ...)\n",
    "\n",
    "Variations:\n",
    "\n",
    "  * 2.1 - Output the description in multiple languages\n",
    "  * 2.2 - \n",
    "    \n",
    "## Challenge\n",
    "\n",
    "## Sample Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22770ba7-a8a4-4e21-964c-e5da3c90714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "landseer_url = \"https://framemark.vam.ac.uk/collections/2006AU1447/full/full/0/default.jpg\"\n",
    "landseer_image_pil = Image.open(requests.get(landseer_url, stream=True).raw)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.PILToTensor()])\n",
    "\n",
    "dog_image_torch = transform(landseer_image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d305b49-a50c-4386-bc29-488115e53b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /Users/digitalmediaadmin/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|█████████████████████████████████████| 97.8M/97.8M [00:39<00:00, 2.59MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newfoundland: 16.6%\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(dog_image_torch).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b79367-75cc-41cd-a482-25f62032b551",
   "metadata": {},
   "source": [
    "# Dog Sculpture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de19e72e-451d-4bf0-92c7-c75d5e035207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "jacquemart_url = \"https://framemark.vam.ac.uk/collections/2016JC7374/full/full/0/default.jpg\"\n",
    "jacquemart_image_pil = Image.open(requests.get(jacquemart_url, stream=True).raw)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.PILToTensor()])\n",
    "\n",
    "dog_image_torch = transform(jacquemart_image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d675ae0d-ca46-4ed7-8adc-fa12a13aaa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedestal: 7.6%\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(dog_image_torch).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f7b64-c52a-4b07-a958-62b5644b9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit - Sample implemenetation code based on that of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df5afa-49db-4d30-83f0-5ffd645ef344",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525bd24b-4fbf-43f7-88e9-0c969f9269aa",
   "metadata": {},
   "source": [
    "# See also "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
